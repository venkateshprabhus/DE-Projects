{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e394332b-329e-4f86-ad83-9cd1fd6af9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: Import packages\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "from dotenv import load_dotenv\n",
    "import mysql.connector\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4a1c9380-2628-4592-a5a1-c127f75180c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "80b90a2c-dad9-4712-b761-a218e05db871",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.upload_file('data/source1.csv', 'etl-workflows', 'raw_data/source1.csv')\n",
    "s3.upload_file('data/source1.xml', 'etl-workflows', 'raw_data/source1.xml')\n",
    "s3.upload_file('data/source1.json', 'etl-workflows', 'raw_data/source1.json')\n",
    "s3.upload_file('data/source2.csv', 'etl-workflows', 'raw_data/source2.csv')\n",
    "s3.upload_file('data/source2.xml', 'etl-workflows', 'raw_data/source2.xml')\n",
    "s3.upload_file('data/source2.json', 'etl-workflows', 'raw_data/source2.json')\n",
    "s3.upload_file('data/source3.csv', 'etl-workflows', 'raw_data/source3.csv')\n",
    "s3.upload_file('data/source3.xml', 'etl-workflows', 'raw_data/source3.xml')\n",
    "s3.upload_file('data/source3.json', 'etl-workflows', 'raw_data/source3.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "eb4d84ef-ea45-4fc0-ae77-0859b6686245",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2: Define function to transform data\n",
    "def convert_units(df):\n",
    "    df['height'] = df['height'] * 0.0254  # Convert height to meters\n",
    "    df['weight'] = df['weight'] * 0.453592  # Convert weight to kilograms\n",
    "    return df  # Ensure the modified DataFrame is returned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3522b3a0-349d-4511-888f-dad08e1d45ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 4: Read files and combine dataframe\n",
    "user = []\n",
    "\n",
    "df = pd.read_csv('data/source1.csv')\n",
    "user.append(df)\n",
    "\n",
    "df = pd.read_xml('data/source1.xml')\n",
    "user.append(df)\n",
    "\n",
    "df = pd.read_json('data/source1.json', lines=True)\n",
    "user.append(df)\n",
    "\n",
    "combined_df = pd.concat(user, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1d5cd4f6-c8c4-458b-b3a8-df3cc54f4708",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 5: Transform the units and save\n",
    "transformed_df = convert_units(combined_df)\n",
    "transformed_df.to_csv('data/transformed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "548d004f-c9c1-4751-a312-533145d303cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 6: Set environment file(To avoid hardcoding the credentials)\n",
    "load_dotenv()\n",
    "db_host = os.getenv(\"DB_HOST\")\n",
    "db_user = os.getenv(\"DB_USER\")\n",
    "db_password = os.getenv(\"DB_PASSWORD\")\n",
    "db_port = os.getenv(\"DB_PORT\")\n",
    "db_name = os.getenv(\"DB_NAME\")\n",
    "\n",
    "access_key = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "secret_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "default_region = os.getenv(\"AWS_DEFAULT_REGION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "06337d40-ff6b-4429-8255-65e921aebefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Establish connection to RDS\n",
    "client = mysql.connector.connect(host=db_host, user=db_user, password=db_password, port=db_port)\n",
    "mycursor = client.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cce62999-46c4-4044-849f-5fb592b9dc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Create the database (if it doesn't exist) and use it\n",
    "mycursor.execute(\"CREATE DATABASE IF NOT EXISTS etldb1;\")\n",
    "mycursor.execute(\"use etldb1\")\n",
    "client.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "25bb29e0-c719-469a-87ab-ad36a5f69a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Create the tables (users)\n",
    "mycursor.execute(\"\"\"CREATE TABLE IF NOT EXISTS users (Name VARCHAR(255), height FLOAT, weight FLOAT);\"\"\")\n",
    "client.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ecb836c3-da5e-4c9c-944e-190522c86415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 11: Create the engine for SQLAlchemy\n",
    "engine = sqlalchemy.create_engine(f\"mysql+mysqlconnector://{db_user}:{db_password}@{db_host}:{db_port}/etldb1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2d347881-f086-4b32-bd06-ffb411193421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 12: Read the CSV files into DataFrames(Ignoring as we already have the data in transformed_df dataframe from Step 5)\n",
    "#transformed_df = pd.read_csv(os.path.join(output_path, '*.csv')     \n",
    "\n",
    "# Step 13: Upload DataFrames to the respective tables\n",
    "transformed_df.to_sql(\"users\", engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a07af9ea-2c3c-4504-8e82-db71aa69806c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Name   height   weight\n",
      "0    alex  1.67081  51.2514\n",
      "1    ajay  1.81661  61.9108\n",
      "2   alice  1.76276  69.4132\n",
      "3    ravi  1.73279  64.5643\n",
      "4     joe  1.72187  65.4533\n",
      "5   simon  1.72466  50.9701\n",
      "6   jacob  1.69621  54.7349\n",
      "7   cindy  1.68885  57.8103\n",
      "8    ivan  1.71755  51.7730\n",
      "9    jack  1.74498  55.9279\n",
      "10    tom  1.77292  64.1787\n",
      "11  tracy  1.77825  61.8972\n",
      "12   john  1.72466  50.9701\n"
     ]
    }
   ],
   "source": [
    "# Step 14: Verify the uploads\n",
    "print(pd.read_sql(\"SELECT * FROM users;\", engine))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "fd163a6e-bf97-48ca-81a4-6ad9f912986c",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.upload_file('data/transformed.csv', 'etl-workflows', 'transformed_data/transformed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617c0fc2-abeb-478d-b66d-2ceb9d1ef5a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
